# ETL Automation

Este proyecto presenta un pipeline de ETL (ExtracciÃ³n, TransformaciÃ³n y Carga) completamente automatizado, diseÃ±ado para facilitar la integraciÃ³n de datos desde mÃºltiples fuentes de manera eficiente y precisa. Este ejemplo prÃ¡ctico es ideal para aprender y aplicar ETL en Python, ofreciendo una base sÃ³lida para proyectos de integraciÃ³n y anÃ¡lisis de datos.

## ğŸ“‹ DescripciÃ³n del Proyecto

Este pipeline sigue un proceso ETL completo para mover y preparar datos para su anÃ¡lisis en bases de datos, permitiendo asÃ­ una mayor flexibilidad y escalabilidad en el manejo de la informaciÃ³n. A continuaciÃ³n, se detalla cada etapa:

- **ExtracciÃ³n**: Obtiene datos desde diversas fuentes, como archivos .csv y bases de datos PostgreSQL y MySQL, asegurando que la informaciÃ³n estÃ© centralizada para su posterior procesamiento.
- **TransformaciÃ³n**: Realiza procesos de limpieza y estandarizaciÃ³n de datos, que incluyen la eliminaciÃ³n de valores nulos y la correcciÃ³n de formatos de fecha, a fin de garantizar la consistencia y precisiÃ³n en los datos preparados.
- **Carga**: Exporta los datos ya transformados hacia bases de datos PostgreSQL o MySQL, donde estarÃ¡n listos para su consulta y anÃ¡lisis. Este paso final asegura que los datos estÃ©n en un formato adecuado para su uso por herramientas de anÃ¡lisis o visualizaciÃ³n.
  Este pipeline es adaptable y puede integrarse en flujos de trabajo de anÃ¡lisis de datos para automatizar la gestiÃ³n de informaciÃ³n y mejorar la eficiencia operativa.

## ğŸ§‘â€ğŸ’» Estructura del Proyecto de ETL

```plaintext
etl_automation/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ db_config.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_data.csv
â”‚
â”œâ”€â”€ etl_pipeline.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸ—‚ï¸ DescripciÃ³n de Archivos

1. **etl_pipeline.py**: CÃ³digo principal de Python que realiza las funciones de ExtracciÃ³n, TransformaciÃ³n y Carga.
2. **config/db_config.yaml**: Archivo de configuraciÃ³n para almacenar credenciales y detalles de conexiÃ³n a las bases de datos (PostgreSQL, MySQL).
3. **data/sample_data.csv**: Datos de ejemplo en formato `.csv` para la prueba de la automatizaciÃ³n.
4. **requirements.txt**: LibrerÃ­as necesarias para ejecutar el pipeline de ETL.

## ğŸ› ï¸ Herramientas y LibrerÃ­as

- **Python**: Para scripting.
- **SQLAlchemy**: Conexiones a bases de datos.
- **pandas**: ManipulaciÃ³n de datos.
- **PyYAML**: Lectura de configuraciones.

## âš™ï¸ ConfiguraciÃ³n

1. Clona el repositorio:

   ```plaintext
   git clone https://github.com/tu_usuario/etl_automation.git
   ```

2. Instala las dependencias:

   ```plaintext
   pip install -r requirements.txt
   ```

3. Configura las credenciales en config/db_config.yaml.

## âŒ› EjecuciÃ³n

- Ejecuta el pipeline de ETL:

  ```bash
  python etl_pipeline.py
  ```

## ğŸ¦¿ ExpansiÃ³n

Este proyecto es adaptable. Agrega nuevos casos de extracciÃ³n y carga segÃºn tus necesidades.

## ğŸ«‚ Contribuciones

SiÃ©ntete libre de contribuir a este repositorio o usarlo como base para proyectos de ETL.

<!-- Connect With Me -->

# âœ‰ï¸ Contacto

<a href="https://www.linkedin.com/in/jeanpaulomv/"><img src="https://img.shields.io/badge/jeanpaulomv-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn" height="30"></a>
<a href="https://www.datascienceportfol.io/jeanpaulomv"><img src="https://img.shields.io/badge/Portfolio-255E63?style=for-the-badge&logo=About.me&logoColor=white" alt="Portfolio" height="30"></a>
